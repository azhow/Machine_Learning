{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trabalho_K_Nearest_Neighbors.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2ApULuQOKt_"
      },
      "source": [
        "#Utilização\n",
        "\n",
        "##1) Dê play para baixar os dados de entrada do GitHub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhZAnftr1ZDU",
        "outputId": "531d19d1-bb66-4380-f284-2d12988d6d1a"
      },
      "source": [
        "!git clone https://github.com/azhow/Machine_Learning.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Machine_Learning'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 2 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (5/5), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83DTH6GGPbW4"
      },
      "source": [
        "##2) Definição dos parâmetros\n",
        "\n",
        "Defina aqui os parâmetros para execução dos experimentos:\n",
        "\n",
        "* INPUT_PATH: é o caminho que contém o CSV com os dados de entrada.\n",
        "\n",
        "* OUTPUT_PATH: é o caminho para onde serão exportados os dados de saída.\n",
        "\n",
        "* NORMALIZE: contém as variações de valor para a normalização de valores usando o método min-max.\n",
        "\n",
        "* DISTANCE_FUNCTIONS: possui as diferentes funções de medida de distância.\n",
        "\n",
        "* HOLDOUT_PROB: contém as porcentagens para os diferentes splits de dados de **treino**.\n",
        "\n",
        "* KS: os diferentes valores do hiperparâmetro K para a execução do KNN\n",
        "\n",
        "Dê play após a definição dos novos parâmetros para efetivar as mudanças."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_TkeDfiOFbh"
      },
      "source": [
        "def euclidean_distance(v1, v2):\n",
        "    \"\"\"\n",
        "    Calculates the euclidean distance of a N-dimensional point\n",
        "    \"\"\"\n",
        "    assert len(v1) == len(v2)\n",
        "    distance = 0\n",
        "    for i in range(len(v1)):\n",
        "        distance += (v1[i] - v2[i])**2.0\n",
        "\n",
        "    return math.sqrt(distance)\n",
        "\n",
        "\n",
        "def manhattan_distance(v1, v2):\n",
        "    \"\"\"\n",
        "    Calculates the manhattan distance of a N-dimensional point\n",
        "    \"\"\"\n",
        "    assert len(v1) == len(v2)\n",
        "    distance = 0\n",
        "    for i in range(len(v1)):\n",
        "        distance += abs(v1[i] - v2[i])\n",
        "\n",
        "    return distance\n",
        "\n",
        "\n",
        "# Define experiment set\n",
        "INPUT_PATH = \"./Machine_Learning/KNN/breast_cancer_data.csv\"\n",
        "OUTPUT_PATH = \"./Machine_Learning/KNN/results.csv\"\n",
        "NORMALIZE = [False, True]\n",
        "DISTANCE_FUNCTIONS = [euclidean_distance, manhattan_distance]\n",
        "HOLDOUT_PROB = [0.8]\n",
        "KS = [1, 3, 5, 7, 53, 101, 285]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M1eL7t6Q1cs"
      },
      "source": [
        "##3) Execução\n",
        "\n",
        "Aperte o play para executar o algoritmo KNN com os diversos parâmetros definidos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9ecGI8YD7IF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5122737a-09a0-4356-83e0-7c54bb1d8df5"
      },
      "source": [
        "from collections import Counter, OrderedDict\n",
        "import math\n",
        "import csv\n",
        "import random\n",
        "\n",
        "class DataElement():\n",
        "    \"\"\"\n",
        "    Represents a data element (either training or testing) from the input data\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_row):\n",
        "        \"\"\"\n",
        "        Constructs from CSV line\n",
        "        \"\"\"\n",
        "        self.label = csv_row[-1]\n",
        "        self.attributes = [float(i) for i in csv_row[:-1]]\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Label: %s, Attributes: %s\" % (self.label, self.attributes)\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "\n",
        "    def normalize(self, attribute_ranges):\n",
        "        \"\"\"\n",
        "        Normalize element using the attribute range\n",
        "        \"\"\"\n",
        "        assert len(attribute_ranges) == len(self.attributes)\n",
        "        # For each attribute update its value with normalized value\n",
        "        for attribute_idx in range(len(self.attributes)):\n",
        "            self.attributes[attribute_idx] = attribute_ranges[attribute_idx].calculate_normalized_value(\n",
        "                self.attributes[attribute_idx])\n",
        "\n",
        "\n",
        "class AttributeRange():\n",
        "    \"\"\"\n",
        "    Represents the attribute range\n",
        "    \"\"\"\n",
        "    def __init__(self, attribute_idx, attribute):\n",
        "        self.id = attribute_idx\n",
        "        self.max = max(attribute)\n",
        "        self.min = min(attribute)\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        s = \"ID: {0} Min: {1} Max: {2}\".format(self.id, self.min, self.max)\n",
        "        return s\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "\n",
        "\n",
        "    def calculate_normalized_value(self, value):\n",
        "        return (value - self.min) / (self.max - self.min) \n",
        "\n",
        "\n",
        "class TrainingSet():\n",
        "    \"\"\"\n",
        "    Represents the whole input data\n",
        "    \"\"\"\n",
        "    def __init__(self, training_elements):\n",
        "        \"\"\"\n",
        "        Iniitialize from CSV data\n",
        "        \"\"\"\n",
        "        self.training_elements = training_elements\n",
        "        \n",
        "\n",
        "def read_input_data(input_csv, holdout_prob_training, normalize):\n",
        "    data_elements = []\n",
        "    # Initialize elements\n",
        "    with open(input_csv) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        line_count = 0\n",
        "        for row in csv_reader:\n",
        "            if line_count != 0:\n",
        "                data_elements.append(DataElement(row[1:]))\n",
        "            line_count += 1\n",
        "    \n",
        "    if normalize:\n",
        "        data_elements = normalize_attribute_ranges(data_elements)\n",
        "\n",
        "    random.shuffle(data_elements)\n",
        "\n",
        "    # Calculate label distribution\n",
        "    labels_quantity_dict = calculate_distribution(data_elements)\n",
        "\n",
        "    training_data = {}\n",
        "\n",
        "    # Determines the needed number of samples for each class in the training set\n",
        "    for label in labels_quantity_dict:\n",
        "        labels_quantity_dict[label] = round(labels_quantity_dict[label] * (len(data_elements) * holdout_prob_training))\n",
        "        # Initialize \n",
        "        training_data[label] = []\n",
        "\n",
        "    test_data = []\n",
        "\n",
        "    # For each class in the dataset, add the needed number of samples to the training dataset\n",
        "    for k in labels_quantity_dict:\n",
        "        for el in data_elements:\n",
        "            if (el.label == k) and (len(training_data[k]) < labels_quantity_dict[k]):\n",
        "                training_data[k].append(el)\n",
        "            elif len(training_data[k]) >= labels_quantity_dict[k]:\n",
        "                break\n",
        "\n",
        "    # Add elements to test data\n",
        "    for el in data_elements:\n",
        "        if el not in training_data[el.label]:\n",
        "            test_data.append(el)\n",
        "\n",
        "    final_training_data = []\n",
        "    for i in training_data.values():\n",
        "        final_training_data += i\n",
        "\n",
        "    return [final_training_data, test_data]\n",
        "\n",
        "\n",
        "def calculate_distribution(data_elements):\n",
        "    \"\"\"\n",
        "    Calculates the distribution of the classes present in the data\n",
        "    \"\"\"\n",
        "    data_elements.sort(key=lambda x: x.label)\n",
        "\n",
        "    # Calculate label distribution\n",
        "    strata = OrderedDict()\n",
        "    for el in data_elements:\n",
        "        if el.label not in strata:\n",
        "            strata[el.label] = 1\n",
        "        else:\n",
        "            strata[el.label] += 1\n",
        "\n",
        "    for label in strata:\n",
        "        strata[label] = strata[label] / len(data_elements)\n",
        "\n",
        "    return strata\n",
        "\n",
        "\n",
        "def most_frequent(l):\n",
        "    \"\"\"\n",
        "    Gets the most frequent element in a list\n",
        "    \"\"\"\n",
        "    occurence_count = Counter(l)\n",
        "    return occurence_count.most_common(1)[0][0]\n",
        "\n",
        "\n",
        "def k_nearest_neighbors(k, training_set, test_element, distance_function):\n",
        "    \"\"\"\n",
        "    Runs KNN on the test_element using the input training_set and distance_function with k\n",
        "    \"\"\"\n",
        "    distance_classification_list = []\n",
        "    # For each element calculate its distance to the test element and store its label\n",
        "    for element in training_set.training_elements:\n",
        "        distance_classification_list.append([distance_function(element.attributes, test_element.attributes), element.label])\n",
        "\n",
        "    distance_classification_list.sort(key=lambda x: x[0])\n",
        "    # Sorts, gets the first k elements and creates a list with the labels from these k-first elements\n",
        "    k_nearest_labels = [i[1] for i in distance_classification_list[:k]]\n",
        "\n",
        "    return most_frequent(k_nearest_labels)\n",
        "\n",
        "\n",
        "def calculate_accuracy(test_data, test_results):\n",
        "    \"\"\"\n",
        "    Measures the accuracy of the test data results\n",
        "    \"\"\"\n",
        "    assert len(test_data) == len(test_results)\n",
        "    hits = 0    \n",
        "    for idx in range(len(test_data)):\n",
        "        if test_data[idx].label == test_results[idx]:\n",
        "            hits += 1\n",
        "\n",
        "    return hits/len(test_data)\n",
        "\n",
        "\n",
        "def normalize_attribute_ranges(data_elements):\n",
        "    \"\"\"\n",
        "    Normalize all training element's attributes\n",
        "    \"\"\"\n",
        "    attribute_ranges = []\n",
        "\n",
        "    # Iterate on every attribute of the training set\n",
        "    for attribute_idx in range(len(data_elements[0].attributes)):\n",
        "        attribute_ranges.append(AttributeRange(attribute_idx,\n",
        "            [element.attributes[attribute_idx] for element in data_elements]))\n",
        "\n",
        "    for element in data_elements:\n",
        "        element.normalize(attribute_ranges)\n",
        "\n",
        "    return data_elements\n",
        "\n",
        "\n",
        "def run_experiment(k, holdout_prob, training_data, test_data, distance_f, normalize):\n",
        "    \"\"\"\n",
        "    Run an experiment and returns the relevant information from the experiment\n",
        "    \"\"\"\n",
        "    training_set = TrainingSet(training_data)\n",
        "\n",
        "    test_results = []\n",
        "    for element in test_data:\n",
        "        test_results.append(k_nearest_neighbors(k, training_set, element, distance_f))\n",
        "    \n",
        "    training_strata = calculate_distribution(training_data)\n",
        "    test_strata = calculate_distribution(test_data)\n",
        "    input_strata = calculate_distribution(training_data + test_data)\n",
        "\n",
        "    header = [\"k\", \"holdout\", \"distance_f\", \"normalization\", \"training_data_size\", \"test_data_size\", \"accuracy\"]\n",
        "\n",
        "    for key in input_strata:\n",
        "        header.append(\"input_strata_\"+key)\n",
        "    for key in training_strata:\n",
        "        header.append(\"training_strata_\"+key)\n",
        "    for key in test_strata:\n",
        "        header.append(\"test_strata_\"+key)\n",
        "\n",
        "    data = [k, holdout_prob, distance_f.__name__, normalize, len(training_data), len(test_data), \n",
        "            calculate_accuracy(test_data, test_results)]\n",
        "\n",
        "    for key in input_strata:\n",
        "        data.append(input_strata[key])\n",
        "    for key in training_strata:\n",
        "        data.append(training_strata[key])\n",
        "    for key in test_strata:\n",
        "        data.append(test_strata[key])\n",
        "\n",
        "    return { \"header\": header, \"data\": data }\n",
        "\n",
        "\n",
        "def export_results_to_csv(output_path, header, experiment_results):\n",
        "    \"\"\"\n",
        "    Exports results from experiments to CSV output_path file\n",
        "    \"\"\"\n",
        "    with open(output_path, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        # write the header\n",
        "        writer.writerow(header)\n",
        "\n",
        "        for result in experiment_results:\n",
        "            assert len(header) == len(result)\n",
        "            # write the data\n",
        "            writer.writerow(result)\n",
        "\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def run_all_experiments():\n",
        "    \"\"\"\n",
        "    Runs all experiments\n",
        "    \"\"\"\n",
        "    print(\"Running experiments...\")\n",
        "    \n",
        "    experiment_results = []\n",
        "    header = []\n",
        "    \n",
        "    # Run all experiments (excluding first one that already ran)\n",
        "    for normalize in NORMALIZE:\n",
        "        for distance_f in DISTANCE_FUNCTIONS:\n",
        "            for holdout_prob in HOLDOUT_PROB:\n",
        "                training_data, test_data = read_input_data(INPUT_PATH, holdout_prob, normalize)\n",
        "                for k in KS:\n",
        "                    result = run_experiment(k, holdout_prob, training_data, test_data, distance_f, normalize)\n",
        "                    experiment_results.append(result[\"data\"])\n",
        "                    header = result[\"header\"]\n",
        "    \n",
        "    print(\"Exporting experiments...\")\n",
        "    # Export results\n",
        "    export_results_to_csv(OUTPUT_PATH, header, experiment_results)\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_all_experiments()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running experiments...\n",
            "Exporting experiments...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ponByFERTWb"
      },
      "source": [
        "##4) Saída\n",
        "\n",
        "A saída deverá estar presente no caminho de saída configurado pelo usuário.\n",
        "\n",
        "Na saída existem as seguintes colunas:\n",
        "* k - Valor de K utilizado.\n",
        "* holdout - Valor de holdout utilizado.\n",
        "* distance_f - Função de distância utilizada.\n",
        "* normalization - Indica se os atributos da entrada foram normalizados.\n",
        "* training_data_size - Quantidade de exemplos utilizados para o treinamento do KNN.\n",
        "* test_data_size - Quantidade de entradas utilizadas para o teste.\n",
        "* accuracy - Performance da predição do algoritmo KNN com os dados parâmetros.\n",
        "* input/training/test_strata_X - Indica a porcentagem da strata X presente na entrada, nos dados de treino e nos dados de teste."
      ]
    }
  ]
}